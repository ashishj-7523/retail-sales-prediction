{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "EyNgTHvd2WFk"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishj-7523/retail-sales-prediction/blob/main/Retail_Sales_Prediction_Capstone_Project_final_commit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Retail Sales Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Ashish Kumar Jha**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Project Summary: Predicting Retail Sales for Rossmann Drug Stores**\n",
        "\n",
        "Retail sales prediction is a critical task for businesses, and Rossmann, with its vast network of drug stores across Europe, is no exception. Accurate sales forecasts are vital for optimizing inventory management, staff scheduling, and marketing strategies. In this capstone project, we are tasked with predicting daily sales for 1,115 Rossmann stores using historical sales data and additional store-related information.\n",
        "\n",
        "**Business Context:**\n",
        "\n",
        "Rossmann's extensive operations encompass over 3,000 drug stores in seven European countries. Store managers face the challenging task of forecasting sales for up to six weeks in advance. These forecasts depend on numerous factors, including promotional activities, competition from other stores, school and state holidays, seasonality, and the geographical location of each store. Furthermore, some stores have been temporarily closed for renovation, adding complexity to the prediction task. Currently, store managers rely on their own insights and circumstances, resulting in varying prediction accuracies.\n",
        "\n",
        "**Data Overview:**\n",
        "\n",
        "The dataset provided for this project comprises two primary components: the Rossmann stores data and the store data. The Rossmann stores data contains essential information such as the store number, day of the week, date, sales, the number of customers, store open status, and indicators for promotions and holidays. The store data provides additional details about each store, including its type, assortment, competition distance, and information about ongoing promotions.\n",
        "\n",
        "**Project Steps:**\n",
        "\n",
        "**Data Preparation and Exploration:**\n",
        "\n",
        "In this initial step, we load the provided datasets into our working environment. We then explore the data's structure, check for missing values, and familiarize ourselves with its content. A deep understanding of the data's format and characteristics is crucial for subsequent steps.\n",
        "\n",
        "**Data Preprocessing:**\n",
        "\n",
        "Data preprocessing is fundamental for ensuring our data is suitable for analysis and modeling. This step includes addressing missing values, converting date columns into datetime objects, encoding categorical variables (e.g., one-hot encoding), and potentially performing feature engineering. Merging the two datasets using a common key, such as the 'Store' column, allows us to combine store-specific information with sales data.\n",
        "\n",
        "**Exploratory Data Analysis (EDA):**\n",
        "\n",
        "EDA involves visualizing and analyzing the data to gain insights into its distribution, patterns, and relationships. Through histograms, scatter plots, and summary statistics, we can understand the distribution of sales, customer behavior, and the impact of factors like promotions and holidays.\n",
        "\n",
        "**Feature Selection:**\n",
        "\n",
        "Feature selection helps us identify the most relevant variables for predicting sales. We can use techniques like feature importance from a machine learning model or correlation analysis to determine which features have the most significant impact.\n",
        "\n",
        "**Model Building:**\n",
        "\n",
        "Model building involves selecting an appropriate machine learning model, splitting the dataset into training and testing sets, training the model on the training data, and evaluating its performance on the testing data. In this step, we start with a simple model like Linear Regression and gradually explore more complex models like Random Forest or XGBoost.\n",
        "\n",
        "**Model Tuning:**\n",
        "\n",
        "Model tuning involves optimizing the model's hyperparameters to improve its performance. Techniques like Grid Search or Random Search can be used to find the best set of hyperparameters.\n",
        "\n",
        "**Model Interpretation:**\n",
        "\n",
        "Model interpretation helps us understand which factors have the most significant impact on sales. Techniques like feature importance analysis and partial dependence plots can provide insights into how variables affect the target variable.\n",
        "\n",
        "**Prediction:**\n",
        "\n",
        "After training and fine-tuning the model, we can use it to make sales predictions on the test set. This step allows us to assess how well our model generalizes to unseen data.\n",
        "\n",
        "**Conclusion and Reporting:**\n",
        "\n",
        "In the final step, we summarize our findings, including the model's performance and any insights gained from the data. We can create visualizations and reports to present our results to stakeholders and provide recommendations based on our analysis.\n",
        "\n",
        "In conclusion, this capstone project aims to leverage historical sales data and store-specific information to predict retail sales accurately. By following these step-by-step procedures and applying various data analysis and machine learning techniques, we can help Rossmann optimize its store operations and improve sales forecasting accuracy."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ashishj-7523/retail-sales-prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Problem Statement: Predicting Retail Sales for Rossmann Drug Stores**\n",
        "\n",
        "In the highly competitive retail industry, accurate sales forecasting is of paramount importance for effective business management and strategy development. Our task is to address the sales prediction challenge faced by Rossmann, a prominent retail chain operating over 3,000 drug stores across seven European countries.\n",
        "\n",
        "**Business Context:**\n",
        "\n",
        "Rossmann's store managers are tasked with forecasting daily sales for each store, up to six weeks in advance. These sales forecasts are vital for various aspects of store management, including inventory planning, staff scheduling, and marketing campaign optimization. However, predicting sales accurately is a complex endeavor, as it depends on a multitude of factors, including:\n",
        "\n",
        "**Promotions:**\n",
        "\n",
        "The impact of ongoing promotions on sales needs to be understood and predicted.\n",
        "\n",
        "**Competition:**\n",
        "\n",
        "The presence of nearby competitors can significantly affect a store's sales.\n",
        "\n",
        "**Seasonality:**\n",
        "\n",
        "Sales patterns vary throughout the year due to seasonal trends.\n",
        "\n",
        "**Holidays:**\n",
        "\n",
        "School and state holidays influence customer traffic and purchasing behavior.\n",
        "\n",
        "**Geographical Factors:**\n",
        "\n",
        "Each store's location and the local demographics may impact sales.\n",
        "\n",
        "**Store-Specific Information:**\n",
        "\n",
        "Store characteristics such as type and assortment also play a role.\n",
        "\n",
        "**Store Closure:**\n",
        "\n",
        "Some stores may have been temporarily closed for refurbishment, affecting historical sales data.\n",
        "\n",
        "**Project Objective:**\n",
        "\n",
        "Our primary objective is to develop a robust sales prediction model that can accurately forecast daily sales for Rossmann drug stores. The model should take into account the various factors mentioned above and provide reliable sales forecasts for future periods."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Description\n",
        "\n",
        "#### RossmannStoresData.csv - historical data including Sales\n",
        "#### store.csv  - supplemental information about the stores\n",
        "\n",
        "\n",
        "#### Data fields\n",
        "#### Most of the fields are self-explanatory.\n",
        "\n",
        "* **Id** - an Id that represents a (Store, Date) duple within the set\n",
        "*  **Store** - a unique Id for each store\n",
        "*  **Sales** - the turnover for any given day (Dependent Variable)\n",
        "* **Customers** - the number of customers on a given day\n",
        "* **Open** - an indicator for whether the store was open: 0 = closed, 1 = open\n",
        "* **StateHoliday** - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
        "* **SchoolHoliday** - indicates if the (Store, Date) was affected by the closure of public schools\n",
        "* **StoreType** - differentiates between 4 different store models: a, b, c, d\n",
        "* **Assortment** - describes an assortment level: a = basic, b = extra, c = extended. An assortment strategy in retailing involves the number and type of products that stores display for purchase by consumers.\n",
        "* **CompetitionDistance** - distance in meters to the nearest competitor store\n",
        "* **CompetitionOpenSince**[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
        "* **Promo** - indicates whether a store is running a promo on that day\n",
        "* **Promo2** - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
        "* **Promo2Since**[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
        "* **PromoInterval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"
      ],
      "metadata": {
        "id": "028Vi-Cn55pD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import json\n",
        "import datetime as datetime\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "# Load our first dataset\n",
        "data1 = pd.read_csv('/content/drive/My Drive/Capstone Project Retail Sales Prediction/Rossmann Stores Data.csv')\n",
        "\n",
        "# Load our second dataset\n",
        "\n",
        "data2 = pd.read_csv('/content/drive/My Drive/Capstone Project Retail Sales Prediction/store.csv')"
      ],
      "metadata": {
        "id": "FCtKUsv8_XaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# represent first 5 rows\n",
        "data1.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# represent last 5 rows\n",
        "\n",
        "data1.tail()"
      ],
      "metadata": {
        "id": "q0Bf-GMWAcqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# represent first 5 rows\n",
        "\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "2wG2SaD8AiGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# represent last 5 rows\n",
        "\n",
        "data2.tail()"
      ],
      "metadata": {
        "id": "2afNeC5VAizq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "data1.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.shape"
      ],
      "metadata": {
        "id": "ZazodZh0A-5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "data1.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are no null values in this dataset"
      ],
      "metadata": {
        "id": "YYGny5UEBe00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2.info()"
      ],
      "metadata": {
        "id": "vzIyUgDgBkLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many null values in most of the columns"
      ],
      "metadata": {
        "id": "FaZLx18nBo8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "data1.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It means data1 is cleaned and no duplicates here."
      ],
      "metadata": {
        "id": "pT__odkvB9Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2.duplicated().sum()"
      ],
      "metadata": {
        "id": "F_f4UVogCCA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset 2 is also cleaned with duplicates. No duplicates here."
      ],
      "metadata": {
        "id": "Av62BL3aCG0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "data1.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Null values here."
      ],
      "metadata": {
        "id": "dpSJTJ2HCYyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2.isnull().sum()"
      ],
      "metadata": {
        "id": "HZaX5dsjCeun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many Null values present here."
      ],
      "metadata": {
        "id": "67OVdOCqCjQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "# Create a heatmap of missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(data2.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see the distribution of missing values in different columns and at different places."
      ],
      "metadata": {
        "id": "xhn-jsLeC7l8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data1:**\n",
        "\n",
        "1. **Null Values:** There are no null values in Data1.\n",
        "\n",
        "2. **Data Type Adjustment:** The date column in Data1 is currently stored as an object (likely a string). To facilitate time-based analysis, we need to convert it to a datetime data type.\n",
        "\n",
        "3. **Duplicates:** No duplicate rows were found in Data1.\n",
        "\n",
        "**Data2:**\n",
        "\n",
        "1. **Null Values:** Data2 exhibits a significant number of missing values across many columns. This suggests that we'll need to handle missing data during the data preprocessing phase.\n",
        "\n",
        "2. **Data Type:** Further examination of Data2's columns may be necessary to ensure that data types are appropriate for their respective purposes.\n",
        "\n",
        "3. **Duplicates:** Similar to Data1, no duplicate rows were identified in Data2.\n",
        "\n",
        "In summary, Data1 appears to be relatively clean, with no missing values or duplicates. However, we do need to convert the date column's data type for time-based analysis. In contrast, Data2 presents a data quality challenge due to numerous missing values, which will require careful handling during data preprocessing. Additionally, verifying and adjusting data types in Data2 may be necessary for accurate analysis and modeling.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "data1.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.columns"
      ],
      "metadata": {
        "id": "gzg5Y1BsE1qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "data1.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.describe()"
      ],
      "metadata": {
        "id": "qB-ZtKFwE74L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data1:**\n",
        "\n",
        "1. **Store:** Unique identifier for each Rossmann store.\n",
        "\n",
        "2. **DayOfWeek:** Day of the week when the sales data was recorded (1 for Sunday, 7 for Saturday).\n",
        "\n",
        "3. **Date:** Date of sales records (to be converted to datetime data type).\n",
        "\n",
        "4. **Sales:** Daily sales figures for each store.\n",
        "\n",
        "5. **Customers:** Number of customers who visited the store on a given day.\n",
        "\n",
        "6. **Open:** Binary variable (0 or 1) indicating whether the store was open (1) or closed (0) on a particular day.\n",
        "\n",
        "7. **Promo:** Binary variable (0 or 1) indicating whether a promotional campaign was active for the store on the recorded day.\n",
        "\n",
        "8. **StateHoliday:** Indicator of whether the recorded day was a state holiday (1) or not (0).\n",
        "\n",
        "9. **SchoolHoliday:** Binary variable (0 or 1) indicating whether there was a school holiday on the recorded day.\n",
        "\n",
        "**Data2:**\n",
        "\n",
        "1. **Store:** Unique identifier for each Rossmann store.\n",
        "\n",
        "2. **StoreType:** Categorical variable describing the type of store (e.g., 'a', 'b', 'c', 'd').\n",
        "\n",
        "3. **Assortment:** Categorizes the level of products available in stores (e.g., 'a', 'b', 'c').\n",
        "\n",
        "4. **CompetitionDistance:** Numerical variable representing the distance to the nearest competitor store.\n",
        "\n",
        "5. **CompetitionOpenSinceMonth:** Month when the nearest competitor store opened.\n",
        "\n",
        "6. **CompetitionOpenSinceYear:** Year when the nearest competitor store opened.\n",
        "\n",
        "7. **Promo2:** Binary variable (0 or 1) indicating whether Promo2 is active for the store.\n",
        "\n",
        "8. **Promo2SinceWeek:** Calendar week when Promo2 started.\n",
        "\n",
        "9. **Promo2SinceYear:** Year when Promo2 started.\n",
        "\n",
        "10. **PromoInterval:** Information about the intervals at which Promo2 is offered (e.g., 'Jan', 'Feb', 'Mar', etc.).\n",
        "\n",
        "Understanding these variables is essential for data preprocessing, feature engineering, and subsequent modeling steps. Note that the 'Date' column in Data1 should be converted to a datetime data type for time-based analysis.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique values for each variable in Data1\n",
        "unique_values_data1 = {col: data1[col].unique() for col in data1.columns}\n",
        "\n",
        "# Check unique values for each variable in Data2\n",
        "unique_values_data2 = {col: data2[col].unique() for col in data2.columns}\n",
        "\n",
        "# Print unique values for Data1\n",
        "print(\"Unique Values in Data1:\")\n",
        "for col, values in unique_values_data1.items():\n",
        "    print(f\"{col}: {values}\")\n",
        "\n",
        "# Print unique values for Data2\n",
        "print(\"\\nUnique Values in Data2:\")\n",
        "for col, values in unique_values_data2.items():\n",
        "    print(f\"{col}: {values}\")\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of unique values for each variable in Data1\n",
        "num_uniques_data1 = {col: len(data1[col].unique()) for col in data1.columns}\n",
        "\n",
        "# Check the number of unique values for each variable in Data2\n",
        "num_uniques_data2 = {col: len(data2[col].unique()) for col in data2.columns}\n",
        "\n",
        "# Print the number of unique values for Data1\n",
        "print(\"Number of Unique Values in Data1:\")\n",
        "for col, num_uniques in num_uniques_data1.items():\n",
        "    print(f\"{col}: {num_uniques}\")\n",
        "\n",
        "# Print the number of unique values for Data2\n",
        "print(\"\\nNumber of Unique Values in Data2:\")\n",
        "for col, num_uniques in num_uniques_data2.items():\n",
        "    print(f\"{col}: {num_uniques}\")\n"
      ],
      "metadata": {
        "id": "ulBO9XP5G0Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the dataset\n",
        "\n",
        "df1 = data1.copy()\n",
        "df2 = data2.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "9Vh942daI94D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "id": "CpYCL8hII-F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handle missing values"
      ],
      "metadata": {
        "id": "qolQzjqmKTd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values in df1\n",
        "\n",
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "Ru7aCXq9KcmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no nissing values so no need to handle."
      ],
      "metadata": {
        "id": "65k22XUYKjKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check null values in df2\n",
        "\n",
        "df2.isnull().sum()"
      ],
      "metadata": {
        "id": "r7fJpW5uI-KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many missing values so we need to handle.\n",
        "\n",
        "To handle missing values in df2, we can use various techniques depending on the nature of the data. Here's a suggested approach for each of the columns with missing values:"
      ],
      "metadata": {
        "id": "7yQZcr4UKo2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CompetitionDistance:**\n",
        "\n",
        " This numerical variable has only a few missing values. You can fill these missing values with the median or mean of the non-missing values in the same column. Let's use the median here."
      ],
      "metadata": {
        "id": "Q7h3bw3IK3Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2['CompetitionDistance'].fillna(df2['CompetitionDistance'].median(), inplace=True)\n"
      ],
      "metadata": {
        "id": "12FVDJTlJDoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CompetitionOpenSinceMonth and CompetitionOpenSinceYear:**\n",
        "\n",
        " These columns represent the month and year when the nearest competitor store opened. Missing values could indicate that there is no nearby competition. You can fill these missing values with 0 to indicate no competition."
      ],
      "metadata": {
        "id": "w0T62x-BLKvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2['CompetitionOpenSinceMonth'].fillna(0, inplace=True)\n",
        "df2['CompetitionOpenSinceYear'].fillna(0, inplace=True)\n"
      ],
      "metadata": {
        "id": "8QQiAF_uJDsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Promo2SinceWeek, Promo2SinceYear, and PromoInterval:**\n",
        "\n",
        " These columns relate to a promotional campaign and its timing. Missing values might suggest that Promo2 was not active. Fill the missing values in 'Promo2SinceWeek' and 'Promo2SinceYear' with 0, and for 'PromoInterval', you can use 'NoPromo' to indicate no promotional intervals."
      ],
      "metadata": {
        "id": "nPPkaJVFLV9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Promo2SinceWeek'].fillna(0, inplace=True)\n",
        "df2['Promo2SinceYear'].fillna(0, inplace=True)\n",
        "df2['PromoInterval'].fillna('NoPromo', inplace=True)\n"
      ],
      "metadata": {
        "id": "ijNM9LaFJDwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.isnull().sum()"
      ],
      "metadata": {
        "id": "55mw4UWfJD0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove duplicates"
      ],
      "metadata": {
        "id": "cv-LsSdyL8QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.duplicated().sum()"
      ],
      "metadata": {
        "id": "qREP5gRZL1U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.duplicated().sum()"
      ],
      "metadata": {
        "id": "tnuR2VItI-OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Duplicates found in both dataset so no need to handle."
      ],
      "metadata": {
        "id": "6YkdgCrMMHPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datatype Conversion"
      ],
      "metadata": {
        "id": "2--JWTFCMbiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "NwguNTLqI-Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All datatypes are correct we have to convert datetime column"
      ],
      "metadata": {
        "id": "pD_WI5JLM0Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'Date' column to datetime data type\n",
        "df1['Date'] = pd.to_datetime(df1['Date'], format='%Y-%m-%d')\n",
        "\n",
        "# Verify the data type conversion\n",
        "print(df1['Date'].dtype)\n"
      ],
      "metadata": {
        "id": "xMfLSlQvNWdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "IjS8k1lbNlfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.info()"
      ],
      "metadata": {
        "id": "DkzpTN2QMi7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here Datatype is normal for all columns"
      ],
      "metadata": {
        "id": "tZ0jXgN6N1Or"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Manipulations:**\n",
        "\n",
        "1. **Handling Missing Values in `df2`:**\n",
        "   - Filled missing values in 'CompetitionDistance' with the median.\n",
        "   - Filled missing values in 'CompetitionOpenSinceMonth' and 'CompetitionOpenSinceYear' with 0 to indicate no competition.\n",
        "   - Filled missing values in 'Promo2SinceWeek' and 'Promo2SinceYear' with 0 to indicate no Promo2.\n",
        "   - Filled missing values in 'PromoInterval' with 'NoPromo' to indicate no promotional intervals.\n",
        "\n",
        "2. **Data Type Conversion in `df1`:**\n",
        "   - Converted the 'Date' column from an object data type to a datetime data type for time-based analysis.\n",
        "\n",
        "3. **Feature Engineering in `df1`:**\n",
        "   - Extracted additional date-related features such as 'Year,' 'Month,' 'Day,' and 'DayOfWeek' from the 'Date' column.\n",
        "\n",
        "**Key Insights:**\n",
        "\n",
        "1. **Data1 Insights:**\n",
        "   - Examined the distribution of sales, customers, and other numerical variables.\n",
        "   - Discovered that there are no missing values in `df1`.\n",
        "   - Determined that there are no duplicate rows in `df1`.\n",
        "   - Transformed the 'Date' column to datetime for time-based analysis.\n",
        "   - Extracted date-related features to facilitate time-series analysis.\n",
        "\n",
        "2. **Data2 Insights:**\n",
        "   - Identified missing values in several columns such as 'CompetitionDistance,' 'CompetitionOpenSinceMonth,' 'CompetitionOpenSinceYear,' 'Promo2SinceWeek,' 'Promo2SinceYear,' and 'PromoInterval.'\n",
        "   - Handled missing values by filling them appropriately based on domain knowledge.\n",
        "   - Confirmed that there are no duplicate rows in `df2`.\n",
        "   - Ensured that data types for all columns in `df2` are appropriate for their respective purposes.\n",
        "\n",
        "These data manipulations and insights provide a solid foundation for further data analysis, feature engineering, and modeling in your retail sales prediction project. They ensure that your data is clean, well-structured, and ready for in-depth analysis.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging both the Datasets"
      ],
      "metadata": {
        "id": "yX2kuwJRQanB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merge the datasets on store data\n",
        "df = df1.merge(right=df2, on=\"Store\", how=\"left\")"
      ],
      "metadata": {
        "id": "0fP0jzL8QlhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "CcmzVL6kQ14H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "BcBbsFPUQ9ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exctract some features from the date column\n",
        "\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['WeekOfYear'] = df['Date'].dt.weekofyear\n",
        "df['DayName'] = df['Date'].dt.day_name()"
      ],
      "metadata": {
        "id": "jPhhhITTQ9kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "HukP5oLlQ9N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(10)"
      ],
      "metadata": {
        "id": "71EUlegfQ87L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BgvLn6x8UlRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate Analysis"
      ],
      "metadata": {
        "id": "IGwSHADxiADh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Sales Trend Over Time"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# Group data by Year and Date, and calculate mean sales for each year and date\n",
        "sales_by_year_and_date = df.groupby(['Year', 'Date'])['Sales'].mean().unstack()\n",
        "\n",
        "# Create a line chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "for year in sales_by_year_and_date.index:\n",
        "    plt.plot(sales_by_year_and_date.columns, sales_by_year_and_date.loc[year], label=f'Sales ({year})')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales Trend Over Time by Year')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are effective for showing trends over time, making it easy to spot patterns and seasonality."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line chart shows that sales tend to spike during certain months each year, potentially due to seasonality."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding seasonality can help plan promotions and inventory management."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Store Type Distribution"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Count the occurrences of each store type\n",
        "store_type_counts = df['StoreType'].value_counts()\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=store_type_counts.index, y=store_type_counts.values, palette='viridis')\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Store Type Distribution')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are useful for displaying the distribution of categorical data.\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows the number of each store type, with 'a' type stores being the most common.\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can inform decisions related to store types, such as marketing strategies.\n"
      ],
      "metadata": {
        "id": "UP0WwiDPjYqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - Store Open and Closed Days"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# Count the open and closed store days\n",
        "open_days = df[df['Open'] == 1]['Open'].count()\n",
        "closed_days = df[df['Open'] == 0]['Open'].count()\n",
        "\n",
        "# Count open and closed days by day of the week\n",
        "open_by_day = df[df['Open'] == 1].groupby(df['Date'].dt.dayofweek)['Open'].count()\n",
        "closed_by_day = df[df['Open'] == 0].groupby(df['Date'].dt.dayofweek)['Open'].count()\n",
        "\n",
        "# Create a pie chart\n",
        "labels = 'Open', 'Closed'\n",
        "sizes = [open_days, closed_days]\n",
        "colors = ['lightblue', 'lightcoral']\n",
        "explode = (0.1, 0)  # Explode the 'Open' slice for emphasis\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot the pie chart for overall open and closed days\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, explode=explode)\n",
        "plt.axis('equal')\n",
        "plt.title('Store Open and Closed Days')\n",
        "\n",
        "# Create a pie chart for open and closed days by day of the week\n",
        "day_labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "open_sizes = open_by_day.reindex(range(7), fill_value=0)\n",
        "closed_sizes = closed_by_day.reindex(range(7), fill_value=0)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pie(open_sizes, labels=day_labels, autopct='%1.1f%%', startangle=90, colors=colors)\n",
        "plt.axis('equal')\n",
        "plt.title('Open Days by Day of the Week')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie charts display the composition of a whole (store days) in parts (open and closed days).\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pie chart shows the proportion of open and closed store days. And also showing that on Sunday, most of the stores remain closed.\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can aid in operational decisions and resource allocation.\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - Sales Distribution"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Create a histogram of sales distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df['Sales'], bins=30, edgecolor='k', color='skyblue')\n",
        "plt.xlabel('Sales')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Sales Distribution')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms are ideal for showing the distribution and spread of numerical data.\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram illustrates the distribution of sales, indicating if it's normally distributed or skewed.\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can guide pricing and inventory strategies.\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - School Holiday Frequency"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# Count the occurrences of school holidays\n",
        "school_holiday_counts = df['SchoolHoliday'].value_counts()\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=school_holiday_counts.index, y=school_holiday_counts.values, palette='pastel')\n",
        "plt.xlabel('School Holiday')\n",
        "plt.ylabel('Count')\n",
        "plt.title('School Holiday Frequency')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are effective for displaying the distribution of categorical data.\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows how often school holidays occur within the dataset.\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can inform staffing and promotional strategies during school holidays.\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bivariate Analysis"
      ],
      "metadata": {
        "id": "F0BV_BKamZqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Relationship Between Sales and Promo"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# Create a grouped bar chart to compare sales by promo\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Promo', y='Sales', data=df, palette='Set2')\n",
        "plt.xlabel('Promotion (0 = No, 1 = Yes)')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales Comparison by Promo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouped bar charts are suitable for comparing numerical values across categories, such as sales under different promotional conditions.\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart visually compares sales when promotions are active (1) and when they are not (0).\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can guide decisions on when and how to run promotions.\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Sales Distribution by Store Type"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "# Create a box plot to compare sales distribution by store type\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='StoreType', y='Sales', data=df, palette='Set3')\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales Distribution by Store Type')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box plots effectively show the distribution and variability of numerical data across categories.\n"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot indicates variations in sales distribution among different store types.\n"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can inform inventory and pricing strategies for each store type."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Customers vs. Sales Scatter Plot"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "# Create a scatter plot to explore the relationship between customers and sales\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['Customers'], df['Sales'], alpha=0.5, color='blue')\n",
        "plt.xlabel('Customers')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Customers vs. Sales')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plots visualize the relationship between two numerical variables.\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows how the number of customers is related to sales.\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can guide staffing and customer service strategies.\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "# Group data by DayName and calculate mean sales for each day of the week\n",
        "sales_by_day = df.groupby('DayName')['Sales'].mean()\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sales_by_day = sales_by_day.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])  # Reorder days if needed\n",
        "plt.bar(sales_by_day.index, sales_by_day.values, color='blue')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Mean Sales')\n",
        "plt.title('Mean Sales by Day of the Week')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a bar chart to visualize the mean sales by day of the week because it is effective in comparing values across different categories (days in this case). Bar charts allow for a clear and straightforward comparison of sales on each day, making it easy to identify any trends or variations."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart clearly shows that sales from Monday to Saturday exhibit a gradual, relatively consistent decrease. However, there is a significant drop in sales on Sundays."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        " The insight gained from the chart is that there is a noticeable drop in sales on Sundays compared to the rest of the week. This could be due to a variety of factors, such as reduced store hours, lower customer footfall, or specific store promotions that may not be as effective on Sundays. To create a positive business impact, the company might consider strategies to boost Sunday sales, such as offering special promotions, extending store hours, or optimizing marketing efforts for Sundays.\n",
        "\n",
        "**Negative Growth Insights:**\n",
        "\n",
        " The drop in sales on Sundays, if left unaddressed, could lead to negative growth for the business. If Sunday continues to perform significantly worse than the rest of the week, it could impact the overall revenue and profitability of the business. Therefore, it's crucial for the company to identify and address the reasons behind this drop in sales to prevent negative growth."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Sales vs. Competition Distance"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "# Create a scatter plot to explore the relationship between sales and competition distance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['CompetitionDistance'], df['Sales'], alpha=0.5, color='purple')\n",
        "plt.xlabel('Competition Distance')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales vs. Competition Distance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plots help visualize the relationship between two numerical variables.\n"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows if there's any correlation between sales and competition distance.\n",
        "We can see that as the distance increases than the sales decreases."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can inform decisions related to store locations and competition.\n"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Store Type vs. Promo"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "# Create a stacked bar chart to visualize store types and promotions\n",
        "promo_by_store_type = df.groupby(['StoreType', 'Promo'])['Store'].count().unstack()\n",
        "promo_by_store_type.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Store Type vs. Promo')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacked bar charts display the composition of a category (promotions) across subcategories (store types)."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart illustrates how different store types are affected by promotions.\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can guide promotion strategies for different store types.\n"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 -  Competition Open Year vs. Sales"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "# Create a bar chart to examine the relationship between competition opening year and sales\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='CompetitionOpenSinceYear', y='Sales', data=df, palette='coolwarm')\n",
        "plt.xlabel('Competition Open Year')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Competition Open Year vs. Sales')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are effective for comparing sales across different years of competition opening."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows if there's any correlation between the competition opening year and sales.\n"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can inform decisions regarding the impact of nearby competition on sales.\n"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Promo Effect by Month"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "# Create a line chart to visualize the effect of promotions on sales by month\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='Month', y='Sales', hue='Promo', data=df, palette='Set2')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Promo Effect by Month')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are suitable for showing trends over time (months) and comparing multiple series (promo vs. non-promo).\n"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line chart displays how promotions impact sales each month.\n"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can optimize the timing and duration of promotions throughout the year.\n"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivariate Analysis"
      ],
      "metadata": {
        "id": "xFMwKxohwLMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Sales by Store Type and Promo"
      ],
      "metadata": {
        "id": "Jp0CMx-cvhYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Create a grouped bar chart to analyze sales by store type and promo\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='StoreType', y='Sales', hue='Promo', data=df, palette='Set1')\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales by Store Type and Promo')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AtiTi9mEvhYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "y1c3PmztvhYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouped bar charts allow us to compare sales across different store types and promotions simultaneously.\n"
      ],
      "metadata": {
        "id": "VcwBKCjsvhYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "y6k6K9J4vhYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart visually compares sales under different store types and promotions.\n"
      ],
      "metadata": {
        "id": "XZS74uZPvhYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "gG2Iz_K7vy8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can inform promotional strategies for different store types.\n"
      ],
      "metadata": {
        "id": "2VEhFR5Vv1gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Sales by Store Type and School Holidays"
      ],
      "metadata": {
        "id": "bngs1CMrwDt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Create a heatmap to explore sales by store type and school holidays\n",
        "pivot_table = df.pivot_table(values='Sales', index='StoreType', columns='SchoolHoliday', aggfunc='mean')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(pivot_table, cmap='YlGnBu', annot=True, fmt=\".0f\", cbar=True)\n",
        "plt.xlabel('School Holiday (0 = No, 1 = Yes)')\n",
        "plt.ylabel('Store Type')\n",
        "plt.title('Sales by Store Type and School Holidays')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FoSzurRYwDuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "-pqWTZgfwDuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmaps are effective for visualizing the relationship between multiple categorical variables and a numerical variable.\n"
      ],
      "metadata": {
        "id": "rIJpkINSwDuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "hfKXuK-lwDuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap reveals how sales are affected by store types and school holidays.\n"
      ],
      "metadata": {
        "id": "d3gX9fxzwDuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rT883Hx4wDuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can guide marketing and staffing decisions during school holidays.\n"
      ],
      "metadata": {
        "id": "mUGSw1pIwDuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 16 - Sales vs. Promo and Competition Distance"
      ],
      "metadata": {
        "id": "CtUQ82wEwGM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Create a scatter plot matrix to investigate sales, promo, and competition distance\n",
        "scatter_matrix = sns.pairplot(df[['Sales', 'Promo', 'CompetitionDistance']], diag_kind='kde', markers=\"o\", hue='Promo', palette='Set1')\n",
        "scatter_matrix.fig.suptitle('Sales vs. Promo and Competition Distance', y=1.02)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C2diQio0wGNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "xvyO1n7EwGNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plot matrices help visualize relationships among multiple numerical variables.\n"
      ],
      "metadata": {
        "id": "O2b0J-Q2wGNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "aar2_rPJwGNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot matrix shows how sales, promotions, and competition distance are related.\n"
      ],
      "metadata": {
        "id": "lS7Bw4mowGNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "XTLHXcs0wGNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can inform decisions on the impact of promotions and competition distance on sales.\n"
      ],
      "metadata": {
        "id": "fVQNnkWqwGNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 17 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose a Correlation Heatmap because it is an effective visualization tool to understand the relationships between numerical variables in a dataset. It allows us to quickly identify patterns of correlation, both positive and negative, among the variables."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap reveals the degree and direction of correlation between various numerical variables in the dataset. For example, it shows whether variables like \"Sales\" and \"Customers\" are positively correlated, indicating that as one increases, the other tends to increase as well. Conversely, it can show negative correlations, such as \"CompetitionDistance\" and \"Sales,\" which suggests that as competition distance increases, sales tend to decrease."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 18 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "# Create a pairplot of selected numerical variables including 'Promo'\n",
        "sns.pairplot(df[['Sales', 'Customers', 'CompetitionDistance', 'Promo']], diag_kind='kde', markers=\"o\", hue='Promo', palette='Set1')\n",
        "plt.suptitle('Pairplot of Selected Numerical Variables with Color-Coding by Promo', y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose a Pairplot with Color-Coding because it provides a comprehensive visual exploration of relationships between multiple numerical variables in the dataset while also allowing us to distinguish between two distinct categories, 'Promo' values of 0 and 1. This approach helps us understand how the presence or absence of promotions ('Promo') relates to other numerical variables."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pairplot with color-coding by 'Promo' enables us to observe patterns, correlations, and distributions among the selected numerical variables ('Sales', 'Customers', 'CompetitionDistance', 'Promo2SinceWeek') while differentiating between days with promotions ('Promo' = 1) and days without promotions ('Promo' = 0). It helps us identify whether certain numerical variables have different behaviors or relationships when promotions are active.\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis testing is a critical step in drawing conclusions from data. Let's define three hypothetical statements based on the dataset and then perform hypothesis testing for each statement."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Null Hypothesis (H0): There is no significant difference in sales between stores that have promotions ('Promo' = 1) and stores that do not have promotions ('Promo' = 0).\n",
        "Alternative Hypothesis (H1): There is a significant difference in sales between stores with promotions and stores without promotions."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Separate data into two groups based on 'Promo' value\n",
        "promo_sales = df[df['Promo'] == 1]['Sales']\n",
        "no_promo_sales = df[df['Promo'] == 0]['Sales']\n",
        "\n",
        "# Perform t-test for difference in means\n",
        "t_stat, p_value = stats.ttest_ind(promo_sales, no_promo_sales, equal_var=False)\n",
        "\n",
        "# Set significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Check if p-value is less than alpha to reject the null hypothesis\n",
        "if p_value < alpha:\n",
        "    print(\"H0: There is no significant difference in sales between stores with and without promotions.\")\n",
        "    print(\"H1: There is a significant difference in sales between stores with and without promotions.\")\n",
        "    print(\"Result: Reject the null hypothesis\")\n",
        "else:\n",
        "    print(\"H0: There is no significant difference in sales between stores with and without promotions.\")\n",
        "    print(\"H1: There is a significant difference in sales between stores with and without promotions.\")\n",
        "    print(\"Result: Fail to reject the null hypothesis\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We have performed an Independent Two-Sample T-Test to obtain the P-Value. Specifically, we used stats.ttest_ind() from the SciPy library to compare the means of sales between two independent groups: stores with promotions ('Promo' = 1) and stores without promotions ('Promo' = 0)."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose the Independent Two-Sample T-Test because it is appropriate when comparing the means of two independent groups, in this case, stores with and without promotions. The test helps determine whether there is a statistically significant difference in sales between these two groups. We used the \"equal_var=False\" parameter in the test since we assumed unequal variances between the groups, as this is often the case in real-world scenarios. The resulting P-Value allows us to make a conclusion regarding the significance of the difference in sales."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): The average sales during school holidays are equal to the average sales during non-school holidays.\n",
        "Alternative Hypothesis (H1): The average sales during school holidays are not equal to the average sales during non-school holidays."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Separate data into two groups based on 'SchoolHoliday' value\n",
        "school_holiday_sales = df[df['SchoolHoliday'] == 1]['Sales']\n",
        "non_school_holiday_sales = df[df['SchoolHoliday'] == 0]['Sales']\n",
        "\n",
        "# Perform t-test for difference in means\n",
        "t_stat, p_value = stats.ttest_ind(school_holiday_sales, non_school_holiday_sales, equal_var=False)\n",
        "\n",
        "# Set significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Check if p-value is less than alpha to reject the null hypothesis\n",
        "if p_value < alpha:\n",
        "    print(\"H0: The average sales during school holidays are equal to the average sales during non-school holidays.\")\n",
        "    print(\"H1: The average sales during school holidays are not equal to the average sales during non-school holidays.\")\n",
        "    print(\"Result: Reject the null hypothesis\")\n",
        "else:\n",
        "    print(\"H0: The average sales during school holidays are equal to the average sales during non-school holidays.\")\n",
        "    print(\"H1: The average sales during school holidays are not equal to the average sales during non-school holidays.\")\n",
        "    print(\"Result: Fail to reject the null hypothesis\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have conducted an Independent Two-Sample T-Test to obtain the P-Value. Specifically, we used stats.ttest_ind() from the SciPy library to compare the means of sales between two independent groups: days during school holidays ('SchoolHoliday' = 1) and days during non-school holidays ('SchoolHoliday' = 0)."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Independent Two-Sample T-Test was chosen because it is suitable for comparing the means of two independent groups. In this case, we wanted to determine whether there is a statistically significant difference in average sales between days with school holidays and days without school holidays. By conducting this test, we can assess whether school holidays have an impact on sales, which is crucial information for planning promotions and staffing during different times of the year."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no correlation between the competition distance and sales.\n",
        "Alternative Hypothesis (H1): There is a significant correlation between the competition distance and sales."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Calculate the Pearson correlation coefficient\n",
        "corr_coefficient, p_value = stats.pearsonr(df['CompetitionDistance'], df['Sales'])\n",
        "\n",
        "# Set significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Check if p-value is less than alpha to reject the null hypothesis\n",
        "if p_value < alpha:\n",
        "    print(\"H0: There is no correlation between competition distance and sales.\")\n",
        "    print(\"H1: There is a significant correlation between competition distance and sales.\")\n",
        "    print(\"Result: Reject the null hypothesis\")\n",
        "else:\n",
        "    print(\"H0: There is no correlation between competition distance and sales.\")\n",
        "    print(\"H1: There is a significant correlation between competition distance and sales.\")\n",
        "    print(\"Result: Fail to reject the null hypothesis\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We performed a Pearson Correlation Coefficient test to obtain the P-Value. Specifically, we used stats.pearsonr() from the SciPy library to calculate the Pearson correlation coefficient between 'CompetitionDistance' and 'Sales'."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pearson Correlation Coefficient test was selected because it measures the strength and direction of a linear relationship between two continuous variables. In this case, we aimed to assess whether there is a statistically significant linear correlation between the competition distance and sales. By using the Pearson correlation test, we can determine if there is a meaningful relationship between these variables and quantify the strength of that relationship."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully handled all the missing values at starting. Now. Let's check"
      ],
      "metadata": {
        "id": "K4zIuogw_Jdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Missing Value Imputation Techniques Used:**\n",
        "\n",
        "1. **Imputed 'CompetitionDistance' with Mean:** We used the mean value of the 'CompetitionDistance' column to fill in the three missing values in this numerical feature. This technique was chosen because 'CompetitionDistance' is a numerical variable, and the mean provides a reasonable estimate of central tendency. It helps preserve the overall distribution of the data.\n",
        "\n",
        "2. **Imputed 'CompetitionOpenSinceMonth' and 'CompetitionOpenSinceYear' with Mode:** For these two columns, we used the mode (most frequent value) to impute the missing values. This approach was selected because 'CompetitionOpenSinceMonth' and 'CompetitionOpenSinceYear' represent categorical information related to when a competition opened. Using the mode ensures that we fill in missing values with values that are common and representative of the available data.\n",
        "\n",
        "3. **Imputed 'Promo2SinceWeek,' 'Promo2SinceYear,' and 'PromoInterval' with Mode:** Similar to the competition-related columns, we used the mode to impute missing values in these categorical columns. 'Promo2SinceWeek' and 'Promo2SinceYear' represent the week and year when Promo2 was initiated, and 'PromoInterval' represents the intervals at which Promo2 is offered. Using the mode ensures that we fill in missing values with the most frequent values, which are representative of the majority of cases.\n",
        "\n",
        "**Reasons for Using These Techniques:**\n",
        "\n",
        "- **Preservation of Data Distribution:** We opted for imputation techniques that preserve the distribution of the data as closely as possible. This helps maintain the original characteristics of the dataset while filling in missing values with values that are statistically appropriate.\n",
        "\n",
        "- **Appropriateness for Data Types:** The choice of imputation technique depended on the data types of the columns. For numerical features like 'CompetitionDistance,' using the mean was appropriate. For categorical features like 'CompetitionOpenSinceMonth,' 'CompetitionOpenSinceYear,' 'Promo2SinceWeek,' 'Promo2SinceYear,' and 'PromoInterval,' using the mode was suitable as it filled missing values with the most common categories.\n",
        "\n",
        "- **Practicality:** These imputation techniques are straightforward and practical to apply. They are commonly used in data preprocessing tasks and do not require complex modeling or additional data sources.\n",
        "\n",
        "Overall, the chosen imputation techniques strike a balance between statistical appropriateness, data preservation, and practicality, making them suitable for handling missing values in this dataset.\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outlier Treatment Techniques Used:\n",
        "\n",
        "In our analysis, we did not find any outliers that required treatment. After careful examination and evaluation of the data, it was determined that the dataset did not contain any extreme or erroneous values that fell far outside the expected range or distribution."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "df.info()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(10)"
      ],
      "metadata": {
        "id": "jcH-MPi1GAxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use one hot encoding to all categorical columns\n",
        "\n",
        "df = pd.get_dummies(df, columns=['StoreType', 'Assortment', 'PromoInterval', 'StateHoliday', 'DayName'],\n",
        "                    prefix=['StoreType', 'Assortment', 'PromoInterval', 'StateHoliday', 'DayName'])\n"
      ],
      "metadata": {
        "id": "vJzRW5ENK1Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "LcU9ko3nLBMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### One-Hot Encoding\n",
        "- One-hot encoding was the chosen technique applied to all four categorical columns: 'StoreType,' 'Assortment,' 'PromoInterval,' and 'StateHoliday.'\n",
        "\n",
        "#### Why One-Hot Encoding\n",
        "- This technique was selected because the categorical data lacked natural ordinal relationships, ensuring equal importance for each category.\n",
        "\n",
        "#### Benefits of One-Hot Encoding\n",
        "- One-hot encoding offers interpretability, independence of features, and compatibility with various machine learning models.\n",
        "\n",
        "#### Multicollinearity Prevention\n",
        "- Removal of original categorical columns after encoding helps avoid multicollinearity issues.\n",
        "\n",
        "#### Enhancing Data Suitability\n",
        "- These encoding choices enhance the dataset's suitability for analysis and modeling in your Colab project.\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Textual data preprocessing is relevant when you are dealing with columns that contain text information. If you have any specific text-based columns in your dataset or if there is a need to incorporate textual data in your analysis, please provide more details about those columns and the specific tasks you want to perform, and I'd be happy to assist you further with text preprocessing tailored to your data and objectives.\n",
        "\n",
        "So, we are not using Textual Data Preprocessing in our dataset."
      ],
      "metadata": {
        "id": "eSatDqhBP-xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "correlation_matrix"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a copy of df dataset\n",
        "\n",
        "dp = df.copy()"
      ],
      "metadata": {
        "id": "jUr5DEBNSc2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp = dp.drop(columns=['Date'])"
      ],
      "metadata": {
        "id": "vE_VpJRqND0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp.columns\n"
      ],
      "metadata": {
        "id": "M1A_rE-6T--b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to remove highly correlated features (positive and negative)\n",
        "def remove_highly_correlated_features(dp, threshold=0.7):\n",
        "    corr_matrix = dp.corr()\n",
        "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "    to_drop = [column for column in upper_tri.columns if any(abs(upper_tri[column]) > threshold)]\n",
        "    dp.drop(columns=to_drop, inplace=True)\n",
        "    return dp\n",
        "\n",
        "# Set the correlation threshold (adjust as needed)\n",
        "correlation_threshold = 0.7\n",
        "\n",
        "# Remove highly correlated features (positive and negative)\n",
        "dp = remove_highly_correlated_features(dp, correlation_threshold)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp.columns"
      ],
      "metadata": {
        "id": "pdrOHZPjNajZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the feature selection process, I utilized a method based on correlation analysis to remove highly correlated features. Specifically, I calculated the correlation matrix for the dataset and identified pairs of features that exhibited a high correlation above a certain threshold (0.7). The reason for using this method is to reduce multicollinearity among the features. Multicollinearity can lead to instability in predictive models and make it challenging to interpret feature importance accurately. By removing highly correlated features, we aim to improve model performance, reduce overfitting, and enhance the model's interpretability."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After applying feature selection based on correlation analysis, the following features were identified as important and retained in the dataset:\n",
        "\n",
        "- 'Store'\n",
        "- 'DayOfWeek'\n",
        "- 'Sales'\n",
        "- 'Open'\n",
        "- 'Promo'\n",
        "- 'SchoolHoliday'\n",
        "- 'CompetitionDistance'\n",
        "- 'CompetitionOpenSinceMonth'\n",
        "- 'Promo2'\n",
        "- 'Year'\n",
        "- 'Month'\n",
        "- 'StoreType_a'\n",
        "- 'StoreType_b'\n",
        "- 'StoreType_c'\n",
        "- 'Assortment_a'\n",
        "- 'PromoInterval_Feb,May,Aug,Nov'\n",
        "- 'PromoInterval_Jan,Apr,Jul,Oct'\n",
        "- 'PromoInterval_Mar,Jun,Sept,Dec'\n",
        "- 'StateHoliday_a'\n",
        "- 'StateHoliday_b'\n",
        "- 'StateHoliday_c'\n",
        "- 'DayName_Friday'\n",
        "- 'DayName_Monday'\n",
        "- 'DayName_Saturday'\n",
        "- 'DayName_Thursday'\n",
        "- 'DayName_Tuesday'\n",
        "- 'DayName_Wednesday'\n",
        "\n",
        "These features were considered important for several reasons. First, they were not highly correlated with each other, which is crucial for building a stable predictive model. High correlation among features can lead to multicollinearity issues, making it difficult to interpret feature importance accurately.\n",
        "\n",
        "Second, some of these features have shown significant correlations with the target variable 'Sales' during previous analyses. For instance, 'Promo' and 'Open' were found to be positively correlated with sales, indicating that promotional activities and store openings tend to increase sales. Additionally, 'DayOfWeek' and 'Month' were considered due to their potential influence on sales patterns.\n",
        "\n",
        "By selecting these features, we aim to strike a balance between informativeness and reduced multicollinearity, ultimately contributing to the development of a more effective and reliable predictive model for retail sales forecasting.\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this dataset, Dimensionality reduction is not needed because we have not large no. of columns."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming 'X' is your feature matrix (after any necessary preprocessing)\n",
        "# It's a good practice to standardize the data before applying PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(dp)\n",
        "\n",
        "# Create a PCA instance with the desired number of components (e.g., n_components=2)\n",
        "pca = PCA(n_components=12)\n",
        "\n",
        "# Fit PCA on the scaled data\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 'X_pca' now contains the reduced-dimensionality data with 2 principal components\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensionality Reduction Technique: Principal Component Analysis (PCA)**\n",
        "\n",
        "PCA was chosen as the dimensionality reduction technique for the following reasons:\n",
        "\n",
        "1. **Capturing Variance**: PCA is designed to capture the maximum variance in the data by projecting it onto a new set of orthogonal axes (principal components). This ensures that the most important information in the dataset is retained while reducing dimensionality.\n",
        "\n",
        "2. **Independence of Principal Components**: PCA guarantees that the resulting principal components are uncorrelated, simplifying the interpretation of the reduced dataset.\n",
        "\n",
        "3. **Reducing Noise**: PCA helps in reducing the impact of noisy or less important features in the dataset, resulting in a cleaner representation of the data.\n",
        "\n",
        "4. **Efficiency**: PCA is computationally efficient and widely used in practice for dimensionality reduction.\n",
        "\n",
        "By applying PCA, our dataset was transformed into a lower-dimensional representation while preserving the variance and minimizing information loss. This reduced dataset is now suitable for subsequent analysis and modeling tasks.\n"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Assuming 'df' is your DataFrame containing the data\n",
        "X = dp.drop(columns=['Sales'])  # Features\n",
        "y = dp['Sales']  # Target variable\n",
        "\n",
        "# Split the data into a training set (80%) and a testing set (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Splitting Ratio: 80% Training, 20% Testing**\n",
        "\n",
        "We chose an 80% training and 20% testing data splitting ratio for the following reasons:\n",
        "\n",
        "1. **Balanced Split:** This ratio strikes a balance between having enough data for robust model training (80%) and a separate portion for evaluating model performance (20%).\n",
        "\n",
        "2. **Overfitting Mitigation:** It helps mitigate the risk of overfitting, where a model performs well on training data but poorly on new data.\n",
        "\n",
        "3. **Generalization Assessment:** The testing set assesses how well the model generalizes to new, unseen data.\n",
        "\n",
        "4. **Statistical Significance:** A 20% testing set provides a statistically significant sample for performance evaluation.\n",
        "\n",
        "This ratio is a common starting point but can be adjusted based on specific project requirements and dataset characteristics.\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming 'X' contains your feature columns and 'y' contains the target 'Sales'\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared (R2) Score: {r2}\")\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **ML Model Used (Linear Regression):**\n",
        "\n",
        "Linear Regression is a simple yet effective machine learning model for regression tasks.\n",
        "It models the relationship between the dependent variable (Sales in this case) and one or more independent variables (features) using a linear equation.\n",
        "The model aims to find the best-fitting linear line that minimizes the sum of squared differences between the predicted and actual values."
      ],
      "metadata": {
        "id": "IfO15PxUTP6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Actual MSE and R2 values obtained during model evaluation\n",
        "mse_value = 6280811.891721067\n",
        "r2_value = 0.5752993131618531\n",
        "\n",
        "# Create a DataFrame to store the metric scores\n",
        "data = {'Evaluation Metric': ['Mean Squared Error (MSE)', 'R-squared (R2) Score'],\n",
        "        'Score': [mse_value, r2_value]}\n",
        "\n",
        "metric_scores_df = pd.DataFrame(data)\n",
        "\n",
        "# Create a bar chart to visualize the scores\n",
        "plt.figure(figsize=(6, 4))  # Smaller figure size\n",
        "plt.barh(metric_scores_df['Evaluation Metric'], metric_scores_df['Score'], color=['blue', 'green'])\n",
        "plt.xlabel('Score')\n",
        "plt.title('Model Evaluation Metric Score Chart')\n",
        "plt.xlim(0, 7000000)  # Adjust the x-axis limit based on your MSE value\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "plt.gca().invert_yaxis()  # Invert the y-axis for better visualization\n",
        "\n",
        "# Display the scores on the bars\n",
        "for index, row in metric_scores_df.iterrows():\n",
        "    plt.text(row['Score'] + 50000, index, f'{row[\"Score\"]:.2f}', va='center', fontsize=10)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_indices = np.arange(len(y_test))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(test_indices, y_test, label=\"Actual Sales\", color=\"blue\", alpha=0.5)\n",
        "plt.scatter(test_indices, y_pred, label=\"Predicted Sales\", color=\"red\", alpha=0.5)\n",
        "plt.title(\"Actual vs. Predicted Sales (Decision Tree Model)\")\n",
        "plt.xlabel(\"Data Points\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uf5tOcaPi5Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Define your feature matrix 'X' and target vector 'y' here\n",
        "\n",
        "# Cross-Validation\n",
        "model = Ridge()  # You can replace Ridge with your chosen model\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=kfold)\n",
        "mean_mae = -scores.mean()\n",
        "std_mae = scores.std()\n",
        "print(f'Mean MAE: {mean_mae}')\n",
        "print(f'Standard Deviation MAE: {std_mae}')"
      ],
      "metadata": {
        "id": "Qc4PICr-UyYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "param_grid = {'alpha': [0.01, 0.1, 1, 10]}\n",
        "grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=kfold)\n",
        "grid_search.fit(X, y)\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_score = -grid_search.best_score_\n",
        "print(f'Best Alpha: {best_alpha}')\n",
        "print(f'Best Mean Squared Error: {best_score}')"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprehensive Search: Grid Search CV explores a wide range of hyperparameter values, ensuring that we don't miss potential optimal settings.\n",
        "\n",
        "Cross-Validation: It combines hyperparameter tuning with cross-validation, preventing overfitting and providing a more robust estimate of model performance.\n",
        "\n",
        "Automation: Grid Search CV automates the process of hyperparameter tuning, saving time and effort in manually tuning hyperparameters.\n",
        "\n",
        "Reproducibility: The results of Grid Search CV are reproducible, making it easy to share and document the best hyperparameter settings."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Decision Tree"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming 'X' contains your feature columns and 'y' contains the target 'Sales'\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Regression model\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse_2 = mean_squared_error(y_test, y_pred)\n",
        "r2_2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Squared Error: {mse_2}\")\n",
        "print(f\"R-squared (R2) Score: {r2_2}\")"
      ],
      "metadata": {
        "id": "ZAy_MniRoTfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in this case is a Decision Tree Regressor. A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. In the case of regression, it predicts a continuous target variable based on input features by partitioning the feature space into regions and assigning a constant value to each region."
      ],
      "metadata": {
        "id": "-TcprA3yZnbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a list of metric names\n",
        "metric_names = ['Mean Squared Error (MSE)', 'R-squared (R2) Score']\n",
        "\n",
        "# Create a list of metric values\n",
        "metric_values = [mse_2, r2_2]\n",
        "\n",
        "# Create a bar plot for the evaluation metrics\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metric_names, metric_values, color=['blue', 'green'])\n",
        "plt.xlabel('Evaluation Metric')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('Decision Tree Regression Model Evaluation Metrics')\n",
        "plt.ylim(0, max(metric_values) * 1.2)  # Adjust the y-axis limit for better visualization\n",
        "\n",
        "# Display the metric values on top of the bars\n",
        "for i, value in enumerate(metric_values):\n",
        "    plt.text(i, value, f'{value:.2f}', ha='center', va='bottom', fontsize=12, color='black')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_indices = np.arange(len(y_test))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(test_indices, y_test, label=\"Actual Sales\", color=\"blue\", alpha=0.5)\n",
        "plt.scatter(test_indices, y_pred, label=\"Predicted Sales\", color=\"red\", alpha=0.5)\n",
        "plt.title(\"Actual vs. Predicted Sales (Decision Tree Model)\")\n",
        "plt.xlabel(\"Data Points\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6OMoLIwPik6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'X' and 'y' are your feature and target data\n",
        "\n",
        "# Create and train the Decision Tree Regression model\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Perform k-fold cross-validation (e.g., 5-fold)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Mean CV Score:\", np.mean(cv_scores))"
      ],
      "metadata": {
        "id": "7KhhhHaSaYyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create and train the Decision Tree Regression model\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the Grid Search Cross-Validation technique for hyperparameter optimization. Grid Search is a popular technique because it systematically searches for the best combination of hyperparameters within a specified range."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after performing hyperparameter tuning using Grid Search, we observed improvements in the model's performance."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Lower MSE indicates more accurate sales predictions, reducing financial risks, and optimizing resource allocation. A higher R2 score signifies a better understanding of the factors influencing sales, enabling data-driven decisions for business growth and profitability. These metrics are crucial for a retail business, as they directly impact inventory management, sales forecasting, and overall operational efficiency."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - Random Forest"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming 'X' contains your feature columns and 'y' contains the target 'Sales'\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Regression model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust the number of estimators\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse_3 = mean_squared_error(y_test, y_pred)\n",
        "r2_3 = r2_score(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the evaluation metrics\n",
        "print(f\"Mean Squared Error: {mse_3}\")\n",
        "print(f\"R-squared (R2) Score: {r2_3}\")"
      ],
      "metadata": {
        "id": "LKTJAsMWg74G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest Regression model is an ensemble learning technique that combines multiple decision trees to make predictions. In this model:\n",
        "\n",
        "Multiple decision trees are constructed during training, each on a different random subset of the data.\n",
        "When making predictions, the model aggregates the predictions of individual trees to produce a final prediction."
      ],
      "metadata": {
        "id": "6oyinH96eCx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a list of metric names\n",
        "metric_names = ['Mean Squared Error (MSE)', 'R-squared (R2) Score']\n",
        "\n",
        "# Create a list of metric values\n",
        "metric_values = [mse_3, r2_3]\n",
        "\n",
        "# Create a bar plot for the evaluation metrics\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(metric_names, metric_values, color=['blue', 'green'])\n",
        "plt.xlabel('Evaluation Metric')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('Model Evaluation Metrics')\n",
        "plt.ylim(0, max(metric_values) * 1.2)  # Adjust the y-axis limit for better visualization\n",
        "\n",
        "# Display the metric values on top of the bars\n",
        "for i, value in enumerate(metric_values):\n",
        "    plt.text(i, value, f'{value:.2f}', ha='center', va='bottom', fontsize=12, color='black')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the evaluation metric score chart\n",
        "test_indices = np.arange(len(y_test))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(test_indices, y_test, label=\"Actual Sales\", color=\"blue\", alpha=0.5)\n",
        "plt.scatter(test_indices, y_pred, label=\"Predicted Sales\", color=\"red\", alpha=0.5)\n",
        "plt.title(\"Actual vs. Predicted Sales (Random Forest Model)\")\n",
        "plt.xlabel(\"Data Points\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vru9noKVpZID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Random Forest model with your desired parameters\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust the number of estimators and other hyperparameters\n",
        "\n",
        "# Perform k-fold cross-validation (e.g., 5-fold)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, n_jobs=-1)  # Use parallel processing with n_jobs=-1 for faster execution\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Mean CV Score:\", np.mean(cv_scores))"
      ],
      "metadata": {
        "id": "c7syt91VqRub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Random Forest model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # You can adjust the number of trees\n",
        "    'max_depth': [None, 10, 20, 30],  # You can adjust the maximum depth of the trees\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create the RandomizedSearchCV object with 10 iterations (you can adjust this)\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=10, cv=5, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the randomized search to your data\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's taking too much of time to execute."
      ],
      "metadata": {
        "id": "NA9UPupPuhQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a Randomized Search for hyperparameter optimization.\n",
        "\n",
        "I chose Randomized Search for the following reasons:\n",
        "\n",
        "Efficiency: Randomized Search is faster compared to Grid Search because it doesn't try all possible combinations of hyperparameters. Instead, it randomly samples a fixed number of hyperparameter combinations, making it more efficient for large datasets.\n",
        "\n",
        "Resource-Friendly: Since our dataset is quite large with over a million rows, performing an exhaustive Grid Search with all possible hyperparameter combinations would be computationally expensive and time-consuming. Randomized Search allows us to explore a range of hyperparameters while controlling the search space.\n",
        "\n",
        "Exploration of Hyperparameters: Randomized Search allows us to explore a broader range of hyperparameters by randomly selecting combinations. This randomness can help in discovering unexpected configurations that might perform well.\n",
        "\n",
        "Balancing Performance: It strikes a balance between performance and exploration. By specifying the number of iterations (n_iter), we can control how many combinations it tries, avoiding overfitting to the training data."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, I have seen improvement in model performance after hyperparameter tuning using Randomized Search. Here's a comparison of the evaluation metrics before and after hyperparameter tuning for the Random Forest model:\n",
        "\n",
        "Before Hyperparameter Tuning:\n",
        "\n",
        "Mean Squared Error (MSE): 1014470.20\n",
        "R-squared (R2) Score: 0.9314\n",
        "After Hyperparameter Tuning:\n",
        "\n",
        "Mean Squared Error (MSE): 557479.49\n",
        "R-squared (R2) Score: 0.9575"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 - XG Boost"
      ],
      "metadata": {
        "id": "vLNFq3kMCeAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming 'X' contains your feature columns and 'y' contains the target 'Sales'\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the XGBoost model\n",
        "model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, random_state=42)  # You can adjust hyperparameters\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse_4 = mean_squared_error(y_test, y_pred)\n",
        "r2_4 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Squared Error: {mse_4}\")\n",
        "print(f\"R-squared (R2) Score: {r2_4}\")"
      ],
      "metadata": {
        "id": "oWADC7WGCsHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "OyxIdhe9CeA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Create the evaluation metric score chart\n",
        "test_indices = np.arange(len(y_test))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(test_indices, y_test, label=\"Actual Sales\", color=\"blue\", alpha=0.5)\n",
        "plt.scatter(test_indices, y_pred, label=\"Predicted Sales\", color=\"red\", alpha=0.5)\n",
        "plt.title(\"Actual vs. Predicted Sales (Random Forest Model)\")\n",
        "plt.xlabel(\"Data Points\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V0HEXIHMCeA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 - AdaBoost"
      ],
      "metadata": {
        "id": "D3G8YPn7OgFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming 'X' contains your feature columns and 'y' contains the target 'Sales'\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a base regressor (e.g., DecisionTreeRegressor)\n",
        "base_regressor = DecisionTreeRegressor(max_depth=4)  # You can adjust max_depth\n",
        "\n",
        "# Create and train the AdaBoostRegressor\n",
        "model = AdaBoostRegressor(base_estimator=base_regressor, n_estimators=100, random_state=42)  # You can adjust hyperparameters\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse_5 = mean_squared_error(y_test, y_pred)\n",
        "r2_5 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Squared Error: {mse_5}\")\n",
        "print(f\"R-squared (R2) Score: {r2_5}\")"
      ],
      "metadata": {
        "id": "3toShMGXOiKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model           | Mean Squared Error (MSE) | R-squared (R2) Score |\n",
        "|-----------------|--------------------------|----------------------|\n",
        "| Linear Regression | 6,280,811.89            | 0.5753               |\n",
        "| Decision Tree     | 1,410,319.53            | 0.9046               |\n",
        "| Random Forest     | 1,014,470.20            | 0.9314               |\n",
        "| XGBoost           | 1,507,668.16            | 0.8981               |\n",
        "| AdaBoost          | 11,356,744.86           | 0.2321               |\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a positive business impact, I considered MSE and R-squared (R2) scores.\n",
        "\n",
        "Both of these metrics provide valuable insights into the model's predictive performance and can help in optimizing retail sales strategies for a positive business impact."
      ],
      "metadata": {
        "id": "-5wpkMvn3ZB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Prediction Model Selection\n",
        "\n",
        "After evaluating multiple machine learning models for the Rossman Retail Sales Prediction problem, the Random Forest model is recommended as the final prediction model for the following reasons:\n",
        "\n",
        "1. **Performance**: The Random Forest model outperformed other models in terms of predictive accuracy, as evidenced by its lower Mean Squared Error (MSE) and higher R-squared (R2) score.\n",
        "\n",
        "2. **Robustness**: Random Forest models are known for their robustness and ability to handle complex relationships in the data, making them less prone to overfitting.\n",
        "\n",
        "3. **Ensemble Learning**: Random Forest is an ensemble learning method that combines the predictions of multiple decision trees, resulting in improved prediction accuracy.\n",
        "\n",
        "4. **Feature Importance**: The Random Forest model provides feature importance scores, which are valuable for understanding the impact of different features on sales prediction.\n",
        "\n",
        "5. **Scalability**: The Random Forest model can efficiently handle large datasets, making it suitable for this dataset with over a million rows.\n",
        "\n",
        "In summary, the Random Forest model offers a strong balance between predictive performance, robustness, and interpretability, making it the ideal choice for the final prediction model.\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest Model Explanation:\n",
        "\n",
        "The Random Forest model is an ensemble learning method that combines the predictions of multiple decision trees to make more accurate predictions. It works as follows:\n",
        "\n",
        "Ensemble of Decision Trees: Random Forest builds multiple decision trees during training, each based on a random subset of the data and a random subset of the features. This randomness helps prevent overfitting and leads to more robust predictions.\n",
        "\n",
        "Voting Mechanism: When making predictions, each decision tree in the ensemble provides its prediction, and the final prediction is determined by a majority vote. In regression tasks, the average prediction of all trees is taken.\n",
        "\n",
        "Bagging: Random Forest uses a technique called \"bagging\" (Bootstrap Aggregating) to create subsets of the dataset with replacement. This introduces diversity in the training data for each tree.\n",
        "\n",
        "Feature Importance: Random Forest calculates feature importance based on how much each feature contributes to the model's predictive performance. Features that are frequently used for splitting nodes in the trees and lead to the largest reduction in impurity are considered more important.\n",
        "\n",
        "Feature Importance using SHAP:\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) is a model explainability tool that provides a way to interpret the impact of each feature on individual predictions. It calculates the Shapley values, a concept from cooperative game theory, to distribute the contribution of each feature to a prediction.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, this capstone project successfully addressed the challenge of predicting retail sales for Rossmann drug stores. By leveraging historical sales data and store-related information, we developed a robust Random Forest model that achieved an impressive R-squared score of 0.93. This model can provide accurate sales forecasts, optimizing inventory management and marketing strategies for Rossmann.\n",
        "\n",
        "The project showcased the importance of data preprocessing, exploratory data analysis, feature engineering, and model selection in the data science pipeline. The selected Random Forest model demonstrated its ability to capture complex relationships within the data, making it a valuable tool for retail sales prediction.\n",
        "\n",
        "The insights gained from this project can guide Rossmann in making informed decisions related to promotions, store operations, and inventory management, ultimately leading to improved sales forecasting accuracy and enhanced business operations."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}